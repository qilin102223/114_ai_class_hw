1
00:00:01,410 --> 00:00:02,630
大家好 我是陳俊智

2
00:00:02,820 --> 00:00:06,150
今天要來和大家介紹我們自己從無到有做出來的演算法

3
00:00:08,700 --> 00:00:11,440
這些是接下來我們會講到的部分  分別是

4
00:00:11,850 --> 00:00:16,190
兩種演算法的運作原理以及小規模測試的結果

5
00:00:16,640 --> 00:00:20,470
然後第二個是兩種演算法初次嘗試遇到的問題

6
00:00:20,770 --> 00:00:24,210
不過因為後面內容比較重要，所以只有簡單帶過

7
00:00:25,170 --> 00:00:29,870
第三個的話，就是本次實作主要使用的資料及介紹

8
00:00:30,600 --> 00:00:34,950
第四個兩種演算法面對新的資料集做的改變

9
00:00:35,000 --> 00:00:36,680
因為會有些不同嘛

10
00:00:37,350 --> 00:00:41,000
那第五個就是實作之後運行的結果

11
00:00:41,280 --> 00:00:42,680
第六個就是最後的總結

12
00:00:43,090 --> 00:00:45,970
那我就廢話不多說  開始介紹第一種演算法

13
00:00:46,540 --> 00:00:49,470
它是由黃麒翰提出並由我進行改進的

14
00:00:49,720 --> 00:00:52,800
右邊是當時在討論分類時提出的方法

15
00:00:53,210 --> 00:00:54,080
原理是這樣的

16
00:00:54,370 --> 00:00:56,910
在所有的資料點當中隨機取兩點

17
00:00:57,100 --> 00:00:58,970
然後以兩點為圓心畫圓

18
00:00:59,210 --> 00:01:02,860
讓圓的半徑一直增加  直到兩個圓相切後停止

19
00:01:03,790 --> 00:01:07,780
再將圓內所有的座標點取平均  得到一個最初的中心點

20
00:01:08,280 --> 00:01:12,190
接著以兩中心點畫圓  繼續重複剛才的行為

21
00:01:12,570 --> 00:01:15,460
就能夠得到兩個最終的中心點

22
00:01:16,150 --> 00:01:19,540
最後再把每個座標點與兩中心點做比較

23
00:01:20,120 --> 00:01:22,520
較接近的點分為一組

24
00:01:22,520 --> 00:01:25,350
就可以成功地把一群資料分成兩類了

25
00:01:26,050 --> 00:01:27,490
不過話是這麼說

26
00:01:27,720 --> 00:01:30,820
相信比較細心的人會發現這個算法有問題

27
00:01:31,110 --> 00:01:34,280
第一個是更新中心點不知道要何時停止

28
00:01:34,380 --> 00:01:37,690
這個是當初在討論時  我們少討論到的一環

29
00:01:37,930 --> 00:01:40,570
第二個就是如果要讓半徑不斷擴大

30
00:01:40,710 --> 00:01:42,340
用程式好像很難辦到

31
00:01:42,680 --> 00:01:44,260
為了解決這個兩個問題

32
00:01:44,360 --> 00:01:46,230
我設定迭代上限為 50 次

33
00:01:46,630 --> 00:01:50,620
也就是說他更新了 50 次中心點就會停止

34
00:01:50,960 --> 00:01:53,430
這樣更新中心點的問題就解決了

35
00:01:53,830 --> 00:01:57,230
再來呢  由於要讓半徑不斷增大很困難

36
00:01:57,230 --> 00:02:00,300
不如就直接讓他增大到兩圓相切的時候

37
00:02:00,550 --> 00:02:06,120
換句話來說  他們的最終半徑就是兩點距離除以 2

38
00:02:06,680 --> 00:02:09,810
如此更改過後程式就沒問題了

39
00:02:10,110 --> 00:02:13,120
再來就可以進行小規模測試

40
00:02:13,440 --> 00:02:15,480
在小規模測試當中

41
00:02:15,480 --> 00:02:19,020
我們使用當初上課用的那份 BMI 資料集

42
00:02:19,020 --> 00:02:22,890
因為當初寫完程式時  我們還沒討論要用什麼資料集

43
00:02:23,240 --> 00:02:25,550
所以就拿這個進行測試了

44
00:02:26,670 --> 00:02:29,230
我反覆進行了 20 多次的測試

45
00:02:29,230 --> 00:02:31,250
發現這個分類法不怎麼穩定

46
00:02:31,250 --> 00:02:34,830
當然它有機率能夠讓這個散佈圖完美地分成兩類

47
00:02:35,400 --> 00:02:38,310
但是大多數的情況還是分得十分奇怪

48
00:02:38,950 --> 00:02:41,830
雖然演算邏輯有些問題  不過後面再說

49
00:02:43,750 --> 00:02:45,670
接下來來介紹第二種分類法

50
00:02:46,010 --> 00:02:47,640
它是由黃麒翰提出的

51
00:02:47,830 --> 00:02:48,980
概念大概是這樣

52
00:02:49,460 --> 00:02:51,290
在散佈圖上隨機取兩點

53
00:02:51,620 --> 00:02:56,560
各找一個和兩點最接近的點  並求出他們的中心座標

54
00:02:57,080 --> 00:02:59,050
再刪除剛才用過的點

55
00:02:59,050 --> 00:03:00,970
接著重複剛才的行為

56
00:03:01,300 --> 00:03:04,470
然後迭代會在所有點都耗盡的時候停止

57
00:03:04,770 --> 00:03:09,030
最後再根據其他點與兩中心點的距離來分類

58
00:03:10,440 --> 00:03:13,270
大致上就是這樣  由於感覺不到任何問題

59
00:03:13,270 --> 00:03:15,330
就直接進行小規模測試吧

60
00:03:16,650 --> 00:03:18,290
測試完的結果  有點無奈

61
00:03:18,530 --> 00:03:20,740
比第一種分類法還要多問題

62
00:03:21,070 --> 00:03:22,460
結果大致是這樣的

63
00:03:22,940 --> 00:03:24,000
它仍然不穩定

64
00:03:24,000 --> 00:03:26,560
跑了 50 多次都能看到不同的結果

65
00:03:27,070 --> 00:03:29,370
甚至中心點還有機率會重疊

66
00:03:30,270 --> 00:03:31,960
再來它有一個潛在的問題

67
00:03:32,300 --> 00:03:34,150
就是它的迭代次數有點多

68
00:03:34,150 --> 00:03:36,670
根據前面的程式邏輯可以知道

69
00:03:37,000 --> 00:03:38,680
如果今天的資料量很大

70
00:03:39,120 --> 00:03:41,640
那他要把所有點消掉的次數就會很多

71
00:03:42,110 --> 00:03:44,470
這會是這個演算法的風險之一

72
00:03:45,970 --> 00:03:47,800
雖然身為這次作業的前菜

73
00:03:47,800 --> 00:03:50,100
也就是兩種自創的演算法

74
00:03:50,390 --> 00:03:51,640
他們感覺有點失敗

75
00:03:51,980 --> 00:03:54,810
不過還是容許我端出這次作業的主菜

76
00:03:55,240 --> 00:03:57,640
也就是這兩種演算法的實際運用

77
00:03:58,070 --> 00:04:01,670
所以接下來來介紹一下這次實作的資料集吧

78
00:04:03,220 --> 00:04:05,620
Iris Database  也就是鳶尾花的資料集

79
00:04:05,950 --> 00:04:08,070
它含有 3 種不同鳶尾花的品種

80
00:04:08,430 --> 00:04:10,350
每個品種都有 50 筆資料

81
00:04:10,740 --> 00:04:12,320
以及 4 種不同特徵

82
00:04:12,480 --> 00:04:15,410
也就是花萼和花瓣的長度及寬度

83
00:04:16,100 --> 00:04:18,020
選擇這個資料集的原因在於

84
00:04:18,150 --> 00:04:21,220
很多種演算法在練習時  都使用這個資料集

85
00:04:21,620 --> 00:04:24,830
就連前面報告的同學也有一位是使用這個資料集

86
00:04:25,270 --> 00:04:28,060
所以選定它作為我們的實作對象

87
00:04:28,300 --> 00:04:31,940
而我們的目標是要把這個資料集分成 3 類

88
00:04:33,600 --> 00:04:36,000
不過這時候就要面臨到兩個大問題

89
00:04:36,230 --> 00:04:39,120
一個是我們的分類法只能把資料分成兩類

90
00:04:39,500 --> 00:04:41,320
另一個是他們都很不穩定

91
00:04:42,160 --> 00:04:43,260
面對這兩個問題

92
00:04:43,410 --> 00:04:46,090
我們聰明的組員(? 黃麒翰看了眼資料集

93
00:04:46,330 --> 00:04:49,170
一針見血地提出了這兩種解決辦法

94
00:04:49,860 --> 00:04:51,680
針對結果的不穩定

95
00:04:51,810 --> 00:04:54,210
他拿掉了一開始的隨機取點

96
00:04:54,500 --> 00:04:57,660
考慮到數據點大多都是左下右上分佈的

97
00:04:58,000 --> 00:05:01,020
他決定從左下角和右上角取點

98
00:05:01,600 --> 00:05:03,710
再來對於只能分兩類的問題

99
00:05:03,770 --> 00:05:06,220
他提議只要在進行一次分類即可

100
00:05:06,460 --> 00:05:09,200
因為 3 種特徵的資料都是 50 筆

101
00:05:09,480 --> 00:05:12,750
分類完後一定會有一組的數量比較多

102
00:05:13,060 --> 00:05:15,130
代表仍需進一步的分類

103
00:05:15,790 --> 00:05:17,720
所以第一次分類完過後

104
00:05:17,720 --> 00:05:21,070
只要在對數量比較多的那一組分類就好了

105
00:05:21,280 --> 00:05:24,300
我當下聽到，又不禁對他多了幾分敬佩 (字幕君：蛤?

106
00:05:26,420 --> 00:05:28,580
那麼接下來就是實際的運作了

107
00:05:28,760 --> 00:05:30,880
這邊是方法一做出來的成果

108
00:05:31,120 --> 00:05:33,770
老實說  我知道它看起來不是那麼得清楚

109
00:05:33,770 --> 00:05:36,080
所以就直接看我整理出來的表格吧

110
00:05:37,170 --> 00:05:39,190
這下面有注釋，以免你們看不懂

111
00:05:39,500 --> 00:05:41,620
我這邊只有放單字的字首而已

112
00:05:41,900 --> 00:05:44,260
Sepal 是花萼 Petal 是花瓣

113
00:05:44,540 --> 00:05:47,140
Width 是寬度 Lenght 是長度

114
00:05:47,420 --> 00:05:49,680
買資訊送英文  雖然我不是數學家

115
00:05:49,780 --> 00:05:51,460
不過聽起來還是挺划算的吧 (x

116
00:05:53,020 --> 00:05:56,330
在這個表格中呈現了不同特徵的分類成功率

117
00:05:56,570 --> 00:05:58,320
那什麼是分類成功率呢？

118
00:05:58,530 --> 00:06:00,160
我給它的定義是這樣的

119
00:06:00,500 --> 00:06:03,480
一個 Group 當中最多的那個種類的數量

120
00:06:03,600 --> 00:06:05,670
去除以那個 Group 的個數

121
00:06:06,020 --> 00:06:09,240
可以看到大致上分類成功率都挺高的

122
00:06:09,620 --> 00:06:11,590
不過單看一組沒有用

123
00:06:11,780 --> 00:06:14,420
假設它分類得很極端  一組只有一個點

124
00:06:14,820 --> 00:06:16,940
那它的成功率就是百分之百

125
00:06:17,130 --> 00:06:18,380
為了避免這樣的誤判

126
00:06:18,570 --> 00:06:21,740
我把 3 個 Group 的成功率都算出來取平均

127
00:06:22,310 --> 00:06:25,190
可以看到不同的特徵組合有不同的成功率

128
00:06:25,490 --> 00:06:28,420
我拿最高和最低的出來觀察

129
00:06:30,050 --> 00:06:31,660
首先是成功率最高的這組

130
00:06:31,850 --> 00:06:33,630
可以看到在原始資料的部分

131
00:06:33,770 --> 00:06:36,370
它的種類其實就已經很接近完美了

132
00:06:36,630 --> 00:06:39,940
只有藍色和綠色交界面稍微混和而已

133
00:06:40,380 --> 00:06:42,010
再看到我做出來的圖表

134
00:06:42,160 --> 00:06:44,600
由於右下角那一塊獨自分成一塊

135
00:06:44,890 --> 00:06:46,810
比照圖上的兩個綠色叉叉

136
00:06:46,810 --> 00:06:49,600
可以知道它切的第一刀就把它分開了

137
00:06:50,020 --> 00:06:52,900
再來就是左邊那一大塊中間來一刀

138
00:06:52,930 --> 00:06:54,560
把它分成兩塊

139
00:06:54,940 --> 00:06:58,110
就近乎完美的達成了分成三類的結果

140
00:06:59,550 --> 00:07:01,860
接著來看成功率比較低的這組

141
00:07:02,380 --> 00:07:06,320
從原始資料可以看到藍色綠色已經混成一團了

142
00:07:06,850 --> 00:07:09,150
然後紅色又和他們比較靠近

143
00:07:09,520 --> 00:07:13,270
造就了 Group1 分歪的結果，得到 88%

144
00:07:13,550 --> 00:07:14,850
又因為資料混濁

145
00:07:15,090 --> 00:07:18,930
二三組沒分好，得到了 71% 和 75%

146
00:07:20,140 --> 00:07:22,150
那麼第一個演算法的結果講完了

147
00:07:22,290 --> 00:07:24,550
接著換到第二個演算法的結果

148
00:07:25,300 --> 00:07:27,410
這個是它做出來的結果對照圖

149
00:07:27,700 --> 00:07:29,470
不過它仍然看不清楚

150
00:07:29,470 --> 00:07:30,910
所以接著往下看

151
00:07:32,920 --> 00:07:35,220
這一頁的內容跟前面的相差不大

152
00:07:35,370 --> 00:07:36,230
除了數據以外

153
00:07:36,730 --> 00:07:37,980
我就簡單講一下

154
00:07:37,980 --> 00:07:40,620
具體來看可以看到成功率都有所下降

155
00:07:40,900 --> 00:07:44,500
這次平均成功率最高的只有到 91% 而已

156
00:07:44,810 --> 00:07:47,240
最好和最壞的圖我稍後介紹

157
00:07:47,530 --> 00:07:48,830
那  我給大家一分鐘

158
00:07:48,970 --> 00:07:50,940
可以稍微看一下我的資料

159
00:07:56,530 --> 00:07:58,330
這次分類比較好的特徵

160
00:07:58,330 --> 00:08:00,820
依然是這個分類比較清楚的特徵組

161
00:08:01,430 --> 00:08:03,450
在我做出來的圖上可以看到

162
00:08:03,540 --> 00:08:06,900
它的第一個中心點重疊在左邊綠色的叉叉上

163
00:08:07,190 --> 00:08:11,190
然後很奇怪的分出了第一組，造就了 80% 的成功率

164
00:08:11,580 --> 00:08:14,940
再來第二次分類是把左上和右下分開

165
00:08:15,320 --> 00:08:18,060
由於左上角那一大塊已經被分了一塊走

166
00:08:18,770 --> 00:08:22,180
所以左上和右下的總數量相差不大

167
00:08:22,790 --> 00:08:26,950
在右下角的中心點走到左上之前就停下

168
00:08:27,410 --> 00:08:31,390
成就了第二組的 95% 和第三組的 100%

169
00:08:33,940 --> 00:08:36,000
最後就是最爛的一個分類了

170
00:08:36,360 --> 00:08:37,750
直接來看我做出來的圖

171
00:08:38,280 --> 00:08:40,440
因為左下角的資料數比較少

172
00:08:40,730 --> 00:08:44,240
所以左下的點刪完  它就往上跑了

173
00:08:44,480 --> 00:08:46,640
中心點停留在很奇怪的地方

174
00:08:46,640 --> 00:08:49,340
第一次的分類就切出了十分怪異的一組

175
00:08:49,610 --> 00:08:52,440
再來第二次的中心點也往右上跑了

176
00:08:52,540 --> 00:08:54,740
導致分類效果極差

177
00:08:56,700 --> 00:08:59,240
好了  這樣兩個算法的結果都介紹完了

178
00:08:59,670 --> 00:09:01,880
接著就進入到結果討論的部分

179
00:09:02,360 --> 00:09:05,720
首先  我們來看看這兩個演算法哪個好哪個壞

180
00:09:06,440 --> 00:09:08,170
透過把成功率相減

181
00:09:08,310 --> 00:09:12,540
可以非常明顯地看到第一個演算法優於第二個演算法

182
00:09:14,380 --> 00:09:17,050
至於原因的話  我推測大概是這樣的

183
00:09:17,360 --> 00:09:19,930
影響兩種演算法的成功率的因素

184
00:09:20,120 --> 00:09:21,370
我認為大致如下

185
00:09:21,800 --> 00:09:24,730
第一種演算法只要資料給的足夠分開

186
00:09:24,970 --> 00:09:27,610
並且不同品種之間的分類十分明確

187
00:09:27,800 --> 00:09:30,010
它就能分類出不錯的成果

188
00:09:30,670 --> 00:09:31,970
至於第二種演算法

189
00:09:32,160 --> 00:09:33,460
它很像在吃東西

190
00:09:33,920 --> 00:09:37,040
資料的數量越多  中心點就會待在那邊比較久

191
00:09:37,340 --> 00:09:39,550
不同種類的資料如果比較接近

192
00:09:39,740 --> 00:09:42,380
它在分類的準確度也會降低

193
00:09:42,810 --> 00:09:44,390
而剛開始的取點位置

194
00:09:44,390 --> 00:09:46,740
會影響它最終應該停在哪裡

195
00:09:47,130 --> 00:09:49,910
種種原因干擾下  它比較不穩定

196
00:09:50,100 --> 00:09:53,900
所以會得出第一種演算法比第二種好的結果

197
00:09:55,630 --> 00:09:57,450
雖然我們得出第一種比較好

198
00:09:57,690 --> 00:09:59,610
但它是真的絕對的好嗎？

199
00:09:59,840 --> 00:10:01,470
我認為它還能夠再改進

200
00:10:02,280 --> 00:10:04,870
回頭看看影響它取點的其他因素

201
00:10:04,990 --> 00:10:08,110
也就是一開始就被我們固定住的兩個因素

202
00:10:08,540 --> 00:10:13,060
誰有說過圓的半徑剛好就是兩中心點的距離的一半是最好的呢？

203
00:10:13,350 --> 00:10:14,790
它終究只是一個假設

204
00:10:15,220 --> 00:10:18,150
究竟半徑該怎麼設還是值得討論的

205
00:10:18,480 --> 00:10:20,260
另外就是剛開始的取點

206
00:10:20,450 --> 00:10:24,050
我們只是因為剛好這個資料集的分佈是左下右上

207
00:10:24,290 --> 00:10:26,450
就把起始點設定在那

208
00:10:26,810 --> 00:10:29,190
但不一定每個資料集都是這樣的分佈

209
00:10:30,330 --> 00:10:32,430
我認為未來還有可以改良的方向

210
00:10:32,750 --> 00:10:34,670
可以考慮讓讓圓的半徑有改變

211
00:10:35,010 --> 00:10:37,070
例如說如果半徑的擴大

212
00:10:37,070 --> 00:10:39,380
能讓圓得到超過一定數量的點

213
00:10:39,740 --> 00:10:40,890
那就讓圓擴大

214
00:10:41,150 --> 00:10:42,960
而非永久的固定半徑

215
00:10:43,430 --> 00:10:46,220
另外也可以重新採用原本的隨機取點

216
00:10:46,460 --> 00:10:48,380
借用一下隨機森林的概念

217
00:10:48,380 --> 00:10:50,380
多做幾次隨機取點的分類

218
00:10:50,890 --> 00:10:54,150
再透過投票的方式選出該點最後的分類

219
00:10:54,570 --> 00:10:57,850
這也不失為一種未來可以考慮的方向

220
00:10:58,860 --> 00:11:00,590
那麼 我的報告就到這邊結束了

221
00:11:00,690 --> 00:11:01,550
謝謝大家

